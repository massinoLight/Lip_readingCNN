{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\n",
      "/Users/massino/Desktop/TensorFlow/datasets\n",
      "['.DS_Store', 'North Korea', 'Today', 'America', 'President', 'Default', 'Journalist', 'News', 'Congress']\n"
     ]
    }
   ],
   "source": [
    "#Import une partie de la dataset seulement 14Mo\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_url = pathlib.Path('./datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529\n"
     ]
    }
   ],
   "source": [
    "#On va compter le nombre de données de notre dataset\n",
    "image_count = len(list(data_url.glob('*/*')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 528 files belonging to 8 classes.\n",
      "Using 423 files for training.\n",
      "Found 528 files belonging to 8 classes.\n",
      "Using 105 files for validation.\n",
      "['America', 'Congress', 'Default', 'Journalist', 'News', 'North Korea', 'President', 'Today']\n"
     ]
    }
   ],
   "source": [
    "#prétraitement sur notre jeu de données \n",
    "batch_size = 3\n",
    "img_height = 200\n",
    "img_width = 200\n",
    "\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_url,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  )\n",
    "\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_url,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = val_data.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
